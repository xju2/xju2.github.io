---
title: "Generative Language Model for Simulating Particles Interacting with Matter"
collection: talks
type: Poster
conference: 23rd International Conference on Computing in High Energy and Nuclear Physics (ACAT 2025)
permalink: /talks/2025-09-08
venue: "University of Hamburg"
date: 2025-09-08
modified: 2026-01-27
location: "Hamburg, Germany"
link: https://indico.cern.ch/event/1488410/contributions/6561402/
prensentor: Dr. Xiangyang Ju
---

## Description
The simulation of particle interactions with detectors plays a critical role in understanding the detector performances and optimizing physics analysis. Without the guidance of the first-principle theory, the current state-of-the-art simulation tool, \textsc{Geant4}, exploits phenomenology-inspired parametric models, which must be combined and carefully tuned to experimental observations. The tuning process, even with the help of semi-auto tools like Professor, is laborious.

Generative language models showed outstanding performance in predicting the next tokens for a given prompt. Its capabilities in learning complex language patterns can be potentially leveraged to learn particle interactions from experimental data.

We introduce a Language Model-based framework for simulation particle detectors. In this framework, the particle information and detector hits will be tokenized into discrete numbers. And a transformer will be trained to learn the statistical correlations between the incoming particles and outgoing detector hits. Instead of directly predicting the detector hits, the transformer will predict the outgoing tokens, which then can be detokenized into detector hits. Our approach replaces the regression task with a multiclass classification task, which Transformers perform much better.

In addition to the introduction of a simulation framework, our contribution includes the introduction of a point cloud data-oriented particle tokenizer and a pre-trained GPT-like model for simulating particles interacting with detector materials.


![Model Performance as a function of training steps](2025-09-08.png)
